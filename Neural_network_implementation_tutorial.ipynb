{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we'll go through the neural network implementation code from Chapter 1 in lots more detail. Along the way, we'll explain some potentially confusing python commands and OOP constructs by going through plenty of mini-exercises. \n",
    "\n",
    "We'll be doing object-oriented Python in this study group. This means we'll be making different classes with their own methods (~functions), which can be instantiated to make objects of that class. I've written up a quick review of the central OOP ideas here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Let's start by making the Network class\n",
    "class Network(object):\n",
    "\n",
    "    # The __init__ method is a sort of Python equivalent of a constructor\n",
    "    # Constructors initialize the values of variables of a newly constructed object\n",
    "    # When we make a new network object, we'll pass a certain value of 'sizes' as an arguments\n",
    "    # For example, n1 = Network([2,5,1]) makes a new Network object with the sizes variable set to [2,5,1] which means\n",
    "    # we have 2 input neurons, 5 in hidden layer, 1 in output\n",
    "    # So, we can make networks with different values of sizes which initialise the network differently and the\n",
    "    # sizes argument specifies the number of neurons in each layer of the network\n",
    "    def __init__(self, sizes):\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        # we randomly initialise the biases of the hidden layer and output neurons only (hence sizes[1:])\n",
    "        # the numpy random.randn command generates numbers from a standard normal distribution (mean 0, variance 1)\n",
    "        # This is a use of a list comprehension, which are a powerful tool in python\n",
    "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
    "        # here, we initialise the weights leading from the input to the hidden layer, and \n",
    "        # from the hidden layer to the output. Play around with the indexing and zip command to get a feel for it\n",
    "        self.weights = [np.random.randn(y, x) \n",
    "                        for x, y in zip(sizes[:-1], sizes[1:])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7, 6]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([[ 1.56279143, -0.79528642],\n",
       "        [-0.58178778,  1.32913706],\n",
       "        [-0.84971337, -0.86955482],\n",
       "        [-1.71414351,  0.61172744],\n",
       "        [ 0.83496434, -0.16360056]]),\n",
       " array([[ 1.2995577 ,  0.34253338, -0.96438116, -0.5923    ,  0.86274076]])]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Some useful commands you could try out to help understand what is happening in the above code are here. \n",
    "# Just put print statements in front of output you want to see. Otherwise, only the last output will be displayed\n",
    "# randn command\n",
    "np.random.randn(1,1)\n",
    "np.random.randn(5,1)\n",
    "np.random.randn(5,3)\n",
    "\n",
    "# array indexing\n",
    "sizes = [2,5,1]\n",
    "sizes[1:]\n",
    "sizes[:-1]\n",
    "zip(sizes[:-1], sizes[1:])\n",
    "\n",
    "# list comprehensions\n",
    "# reminder of some simpler uses\n",
    "[x for x in range(10)]\n",
    "[x**2 for x in range(10)]\n",
    "[x**2 for x in range(10) if x%2==0]\n",
    "\n",
    "# leading up to understanding the specific usage in the Network class definition\n",
    "[x+y for x, y in zip(sizes[:-1], sizes[1:])]\n",
    "[x*y for x, y in zip(sizes[:-1], sizes[1:])]\n",
    "[x for x, y in zip(sizes[:-1], sizes[1:])]\n",
    "[[x,y] for x, y in zip(sizes[:-1], sizes[1:])]\n",
    "[np.random.randn(x) for x, y in zip(sizes[:-1], sizes[1:])]\n",
    "[np.random.randn(x, y) for x, y in zip(sizes[:-1], sizes[1:])]\n",
    "[np.random.randn(y, x) for x, y in zip(sizes[:-1], sizes[1:])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "[array([[ 0.1265344 ],\n",
      "       [ 0.92402966],\n",
      "       [-1.62969926],\n",
      "       [ 1.62903212],\n",
      "       [ 0.41219127]]), array([[ 0.83005241]])]\n",
      "[array([[-0.59216698,  0.57756099],\n",
      "       [-1.05164244,  1.77603307],\n",
      "       [ 0.7664931 ,  0.05062822],\n",
      "       [-1.40892278,  0.44292187],\n",
      "       [-2.44369652,  1.38992025]]), array([[-1.75631984, -1.43950517, -0.39580496, -1.01950956, -0.03196487]])]\n"
     ]
    }
   ],
   "source": [
    "# Let's go ahead and make a new network\n",
    "net1 = Network([2,5,1])\n",
    "print net1.num_layers\n",
    "print net1.biases\n",
    "print net1.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's make the sigmoid function\n",
    "# Refer to the textbook for an explanation of the function's shape (an S, a sort of smoothed step function)\n",
    "def sigmoid(z):\n",
    "    \"\"\"The sigmoid function.\"\"\"\n",
    "    return 1.0/(1.0+np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4.5397868702434395e-05"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check sigmoid function works as intended\n",
    "print sigmoid(0)\n",
    "sigmoid(1)\n",
    "sigmoid(-1)\n",
    "sigmoid(10)\n",
    "sigmoid(-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Next up, let's make the feedforward method of the Network class \n",
    "# we'll put this code into the correct place later, let's understand it first\n",
    "# We are trying to apply the sigmoid function to the input from each layer\n",
    "# I.e. each layer applies the sigmoid function on the layer before it\n",
    "# I got rid of the references to 'self' for now, for clarity\n",
    "def feedforward(a):\n",
    "    for b, w in zip(biases, weights):\n",
    "        print \"The weights we're working with are:\"; print w\n",
    "        print \"The biases we're working with are:\"; print b\n",
    "\n",
    "        a = sigmoid(np.dot(w, a)+b)\n",
    "        print \"We put the weighted input + bias into the sigmoid function, which for this layer gives activations of:\",; print a; print \"done printing a\"\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.59145897843278006"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# So we can see what's happening, let's play with numbers\n",
    "biases = net1.biases\n",
    "weights = net1.weights\n",
    "# recall these are the biases of the 5 hidden layers and the 1 output layer\n",
    "biases\n",
    "# these are the weights leading from each of the 2 neurons in the input to each of the 5 neurons in the hidden layer\n",
    "# and the weights from the 5 input neurons to the 1 output neuron\n",
    "weights\n",
    "# Recall that for each e.g. hidden layer neuron, we need to multiply all incoming input by weights and sum it up and add a bias\n",
    "# Then we put this value through the sigmoid function\n",
    "# So, we need an efficient way of multiplying a lot of weights and a lot of inputs (activations) together\n",
    "# We'll use the dot product for this, np.dot(weights, inputs)\n",
    "# let's see what np.dot does\n",
    "np.dot([1,1,1], [5,0,1])\n",
    "np.dot([3,1,2], [0,0,0])\n",
    "# looks good to me! We multiply element wise and add these values up\n",
    "# we can immediately add a bias to the output\n",
    "np.dot([3,3,3], [1,1,1]) + 1\n",
    "# then we'll want to apply the sigmoid function\n",
    "z = np.dot([3,3,3], [1,1,1]) + 1\n",
    "sigmoid(z)\n",
    "# that's quite a high input to the sigmoid function, let's try something more realistic\n",
    "z = np.dot([-0.3,0.2,0.5], [0.4,0.2,-0.1]) + 0.5\n",
    "sigmoid(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# what on earth does zip(biases, weights) look like? \n",
    "# if you're also finding it hard to visualise all these different matrices, try:\n",
    "import pandas as pd\n",
    "pd.DataFrame([1,2])\n",
    "pd.DataFrame([3,4])\n",
    "pd.DataFrame(zip([1,2],[3,4]))\n",
    "# now we can look at these more clearly:\n",
    "pd.DataFrame(biases)\n",
    "pd.DataFrame(weights)\n",
    "pd.DataFrame(zip(biases,weights))\n",
    "# finally, try to understand:\n",
    "biases\n",
    "weights\n",
    "zip(biases, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hope that helps! Let's move on to the mini-batch stochastic gradient descent function\n",
    "# We will have to take training data and the desired outputs and move the weights and bias vectors towards values \n",
    "# that get close to computing the desired output\n",
    "# I am ignoring the test_data option and the self references for now.\n",
    "def SGD(training_data, epochs, mini_batch_size, eta, test_data=None):\n",
    "\n",
    "    n = len(training_data)\n",
    "    # the xrange function is very similar to range(generates integers from 0 to whatever) but doesn't generate a static list at run time\n",
    "    # it only generates integers when they're needed, this is good when lists are large and the system is memory sensitive\n",
    "    for j in xrange(epochs):\n",
    "        # random.shuffle shuffles the training data in place\n",
    "        random.shuffle(training_data)\n",
    "        # let's create our mini batches of size mini_batch_size\n",
    "        mini_batches = [\n",
    "            training_data[k:k+mini_batch_size]\n",
    "            for k in xrange(0, n, mini_batch_size)]\n",
    "        # then, for each mini_batch, we update\n",
    "        for mini_batch in mini_batches:\n",
    "            self.update_mini_batch(mini_batch, eta)\n",
    "        print \"Epoch {0} complete\".format(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[5, 6], [1, 2], [3, 4]]"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first, let's try out the random.shuffle function\n",
    "# notice the function doesn't return anything, but if you check back on x after shuffling, it's been shuffled\n",
    "x = [1,2,3,4]\n",
    "random.shuffle(x)\n",
    "x\n",
    "# now try in 2 dimensions like with our training input\n",
    "x = [[1,2], [3,4], [5,6]]\n",
    "random.shuffle(x)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[1, 1], [1, 2]], [[2, 3], [3, 4]], [[5, 7], [7, 8]]]"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Next up, let's get to grips with how the mini batches are created\n",
    "# let's make up some specific values for variables so we can follow them through\n",
    "training_data = [[1,1], [1,2], [2,3], [3,4], [5,7], [7,8]]\n",
    "n = len(training_data)\n",
    "mini_batch_size = 2\n",
    "\n",
    "# recall that range and xrange work by specifying the starting value, the end value, and the step size\n",
    "# so xrange(0, n, mini_batch_size), in our case xrange(0,6,2), will produce the output [0,2,4]\n",
    "xrange(0, n, mini_batch_size)\n",
    "range(0, n, mini_batch_size)\n",
    "\n",
    "# this produces start indices which we can use to split our shuffled training data set on\n",
    "training_data[0:0+mini_batch_size]\n",
    "training_data[2:2+mini_batch_size]\n",
    "\n",
    "# all together, let's now run:\n",
    "# this divides up our training data set of 6 data points into mini-batches of 2 data points each\n",
    "mini_batches = [training_data[k:k+mini_batch_size]\n",
    "                for k in xrange(0, n, mini_batch_size)]\n",
    "\n",
    "mini_batches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Great! For each step in the training phase (each 'epoch'), you'll have noticed that we updated each mini_batch with the\n",
    "# update_mini_batch method. Now let's take a look at what this is \n",
    "# Intuitively, we want to gradually move the weights and biases towards the values that produce correct output\n",
    "# The rate at which we move towards this optimum depends on the learning rate, eta\n",
    "\n",
    "def update_mini_batch(mini_batch, eta):\n",
    "    # 'nabla' is the word for the upside down triangle symbol we've been using to denote gradient vectors\n",
    "    #np.zeros produces zeros in the same shape as the biases and weights\n",
    "    nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "    nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "    \n",
    "    for x, y in mini_batch:\n",
    "        \n",
    "        delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
    "        # we update the gradient vectors\n",
    "        nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "        nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "    self.weights = [w-(eta/len(mini_batch))*nw \n",
    "                    for w, nw in zip(self.weights, nabla_w)]\n",
    "    self.biases = [b-(eta/len(mini_batch))*nb \n",
    "                    for b, nb in zip(self.biases, nabla_b)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 2)"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try out the zeros function\n",
    "np.zeros(5)\n",
    "np.zeros([2,5])\n",
    "# try out the shape function\n",
    "x = np.array([1,2,3])\n",
    "x.shape\n",
    "x = np.array([[1,2],[4,5]])\n",
    "x.shape\n",
    "x = np.array([[1,2,3],[4,5,6]])\n",
    "x\n",
    "x.shape\n",
    "# trying it out on our data set\n",
    "weights\n",
    "weights[0].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
