{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we'll be going through the neural network implementation code from Chapter 1 in more detail. Along the way, a series of mini-exercises should help to explain some of the potentially confusing commands.\n",
    "\n",
    "We'll be doing a bit of object-oriented Python in this study group - this means we'll be making different classes with their own methods (~functions), which can be instantiated to make objects of that class. See the email and repo for a quick review of some key ideas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Let's start by making the Network class\n",
    "class Network(object):\n",
    "\n",
    "    # The __init__ method is a sort of Python equivalent of a constructor\n",
    "    # Constructors initialize the values of variables of a newly constructed object\n",
    "    # When we make a new network object, we'll pass a certain value of 'sizes' as an arguments\n",
    "    # For example, n1 = Network([2,5,1]) makes a new Network object with the sizes variable set to [2,5,1] which means\n",
    "    # we'll have 2 input neurons, 5 in hidden layer, 1 in output\n",
    "    # So, we can make networks with different values of sizes which initialise the network differently and the\n",
    "    # sizes argument specifies the number of neurons in each layer of the network\n",
    "    def __init__(self, sizes):\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        # we randomly initialise the biases of the hidden layer and output neurons only (hence sizes[1:])\n",
    "        # the numpy random.randn command generates numbers from a standard normal distribution (mean 0, variance 1)\n",
    "        # This is a use of a list comprehension, which are a powerful tool in python\n",
    "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
    "        # here, we initialise the weights leading from the input to the hidden layer, and \n",
    "        # from the hidden layer to the output. Play around with the indexing and zip command to get a feel for it\n",
    "        self.weights = [np.random.randn(y, x) \n",
    "                        for x, y in zip(sizes[:-1], sizes[1:])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-1.08351413, -1.44065192],\n",
       "        [ 0.32184529, -0.64573423],\n",
       "        [-1.03064089,  0.54424213],\n",
       "        [ 0.06212166,  1.41520053],\n",
       "        [-0.15007144,  0.98580898]]),\n",
       " array([[-0.13527947,  1.18161082,  0.5471675 , -0.53280803,  0.04046108]])]"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Some useful commands you could try out to help understand what is happening in the above code\n",
    "# Click \"insert cell below\" to get a new box you can type the commands into\n",
    "# Typing commands yourself > copying/pasting commands or typing print in front of commands > reading\n",
    "\n",
    "# randn command\n",
    "np.random.randn(1,1)\n",
    "np.random.randn(5,1)\n",
    "np.random.randn(5,3)\n",
    "\n",
    "# array indexing\n",
    "sizes = [2,5,1]\n",
    "sizes[1:]\n",
    "sizes[:-1]\n",
    "zip(sizes[:-1], sizes[1:])\n",
    "\n",
    "# list comprehensions\n",
    "# reminder of some simpler uses\n",
    "[x for x in range(10)]\n",
    "[x**2 for x in range(10)]\n",
    "[x**2 for x in range(10) if x%2==0]\n",
    "\n",
    "# leading up to understanding the specific usage in the Network class definition\n",
    "[x+y for x, y in zip(sizes[:-1], sizes[1:])]\n",
    "[x*y for x, y in zip(sizes[:-1], sizes[1:])]\n",
    "[x for x, y in zip(sizes[:-1], sizes[1:])]\n",
    "[[x,y] for x, y in zip(sizes[:-1], sizes[1:])]\n",
    "[np.random.randn(x) for x, y in zip(sizes[:-1], sizes[1:])]\n",
    "[np.random.randn(x, y) for x, y in zip(sizes[:-1], sizes[1:])]\n",
    "[np.random.randn(y, x) for x, y in zip(sizes[:-1], sizes[1:])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "[array([[ 0.1265344 ],\n",
      "       [ 0.92402966],\n",
      "       [-1.62969926],\n",
      "       [ 1.62903212],\n",
      "       [ 0.41219127]]), array([[ 0.83005241]])]\n",
      "[array([[-0.59216698,  0.57756099],\n",
      "       [-1.05164244,  1.77603307],\n",
      "       [ 0.7664931 ,  0.05062822],\n",
      "       [-1.40892278,  0.44292187],\n",
      "       [-2.44369652,  1.38992025]]), array([[-1.75631984, -1.43950517, -0.39580496, -1.01950956, -0.03196487]])]\n"
     ]
    }
   ],
   "source": [
    "# Let's go ahead and make a new network\n",
    "net1 = Network([2,5,1])\n",
    "print net1.num_layers\n",
    "print net1.biases\n",
    "print net1.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's make the sigmoid function\n",
    "# Refer to the textbook for an explanation of the function's shape (an S, a sort of smoothed step function)\n",
    "def sigmoid(z):\n",
    "    return 1.0/(1.0+np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.5397868702434395e-05"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check sigmoid function works as intended\n",
    "sigmoid(0)\n",
    "sigmoid(1)\n",
    "sigmoid(-1)\n",
    "sigmoid(10)\n",
    "sigmoid(-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Next up, let's make the feedforward method of the Network class \n",
    "# we'll put this code into the correct place later, let's understand it first\n",
    "# We are trying to apply the sigmoid function to the input from each layer\n",
    "# I.e. each layer applies the sigmoid function on the layer before it\n",
    "# I got rid of the references to 'self' for now, for clarity\n",
    "def feedforward(a):\n",
    "    for b, w in zip(biases, weights):\n",
    "        print \"The weights we're working with are:\"; print w\n",
    "        print \"The biases we're working with are:\"; print b\n",
    "        a = sigmoid(np.dot(w, a)+b)\n",
    "        print \"We put the weighted input + bias into the sigmoid function, which for this layer gives activations of:\",; print a; print \"done printing a\"\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.59145897843278006"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# So we can see what's happening, let's play with numbers\n",
    "biases = net1.biases\n",
    "weights = net1.weights\n",
    "# recall these are the biases of the 5 hidden layers and the 1 output layer\n",
    "biases\n",
    "# these are the weights leading from each of the 2 neurons in the input to each of the 5 neurons in the hidden layer\n",
    "# and the weights from the 5 input neurons to the 1 output neuron\n",
    "weights\n",
    "# Recall that for each e.g. hidden layer neuron, we need to multiply all incoming input by weights and sum it up and add a bias\n",
    "# Then we put this value through the sigmoid function\n",
    "# So, we need an efficient way of multiplying a lot of weights and a lot of inputs (activations) together\n",
    "# We'll use the dot product for this, np.dot(weights, inputs)\n",
    "# let's see what np.dot does\n",
    "np.dot([1,1,1], [5,0,1])\n",
    "np.dot([3,1,2], [0,0,0])\n",
    "# looks good to me! We multiply element wise and add these values up\n",
    "# we can immediately add a bias to the output\n",
    "np.dot([3,3,3], [1,1,1]) + 1\n",
    "# then we'll want to apply the sigmoid function to get the activation of the next layer of neurons\n",
    "z = np.dot([3,3,3], [1,1,1]) + 1\n",
    "sigmoid(z)\n",
    "# that's quite a high input to the sigmoid function, let's try something more realistic\n",
    "z = np.dot([-0.3,0.2,0.5], [0.4,0.2,-0.1]) + 0.5\n",
    "sigmoid(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(array([[ 0.1265344 ],\n",
       "         [ 0.92402966],\n",
       "         [-1.62969926],\n",
       "         [ 1.62903212],\n",
       "         [ 0.41219127]]), array([[-0.59216698,  0.57756099],\n",
       "         [-1.05164244,  1.77603307],\n",
       "         [ 0.7664931 ,  0.05062822],\n",
       "         [-1.40892278,  0.44292187],\n",
       "         [-2.44369652,  1.38992025]])),\n",
       " (array([[ 0.83005241]]),\n",
       "  array([[-1.75631984, -1.43950517, -0.39580496, -1.01950956, -0.03196487]]))]"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# what on earth does zip(biases, weights) look like? \n",
    "# if you're also finding it hard to visualise all these different matrices, try:\n",
    "import pandas as pd\n",
    "pd.DataFrame([1,2])\n",
    "pd.DataFrame([3,4])\n",
    "pd.DataFrame(zip([1,2],[3,4]))\n",
    "# now we can look at these more clearly:\n",
    "pd.DataFrame(biases)\n",
    "pd.DataFrame(weights)\n",
    "pd.DataFrame(zip(biases,weights))\n",
    "# finally, try to understand:\n",
    "biases\n",
    "weights\n",
    "zip(biases, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hope that helps! Let's move on to the mini-batch stochastic gradient descent function\n",
    "# We will have to take training data and the desired outputs and move the weights and bias vectors towards values \n",
    "# that get close to computing the desired output\n",
    "# I am ignoring the test_data option and the self references \n",
    "def SGD(training_data, epochs, mini_batch_size, eta, test_data=None):\n",
    "\n",
    "    n = len(training_data)\n",
    "    \n",
    "    # the xrange function is very similar to range(generates integers from 0 to whatever-1) but doesn't generate a static list at run time\n",
    "    # it only generates integers when they're needed, this is good when lists are large and the system is memory sensitive\n",
    "    for j in xrange(epochs):\n",
    "        \n",
    "        # random.shuffle shuffles the training data in place\n",
    "        random.shuffle(training_data)\n",
    "        \n",
    "        # let's create our mini batches of size mini_batch_size\n",
    "        mini_batches = [\n",
    "            training_data[k:k+mini_batch_size]\n",
    "            for k in xrange(0, n, mini_batch_size)]\n",
    "        \n",
    "        # then, for each mini_batch, we \"update\" (i.e. compute gradients and update weights and biases)\n",
    "        for mini_batch in mini_batches:\n",
    "            self.update_mini_batch(mini_batch, eta)\n",
    "        \n",
    "        # an epoch is complete when all mini-batches have been processed\n",
    "        print \"Epoch {0} complete\".format(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[5, 6], [1, 2], [3, 4]]"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Decomposing the SGD function\n",
    "# first, let's try out the random.shuffle function\n",
    "# notice the function doesn't return anything, but if you check back on x after shuffling, it's been shuffled\n",
    "x = [1,2,3,4]\n",
    "random.shuffle(x)\n",
    "x\n",
    "# now try in 2 dimensions like with our training input\n",
    "x = [[1,2], [3,4], [5,6]]\n",
    "random.shuffle(x)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Next up, let's get to grips with how the mini batches are created\n",
    "# let's make up some specific values for variables so we can follow them through\n",
    "training_data = [[1,1], [1,2], [2,3], [3,4], [5,7], [7,8]]\n",
    "n = len(training_data)\n",
    "mini_batch_size = 2\n",
    "\n",
    "# recall that range and xrange work by specifying the starting value, the end value, and the step size\n",
    "# so xrange(0, n, mini_batch_size), in our case xrange(0,6,2), will produce the output [0,2,4]\n",
    "xrange(0, n, mini_batch_size)\n",
    "range(0, n, mini_batch_size)\n",
    "\n",
    "# this produces start indices which we can use to split our shuffled training data set on\n",
    "training_data[0:0+mini_batch_size]\n",
    "training_data[2:2+mini_batch_size]\n",
    "\n",
    "# all together, let's now run:\n",
    "# this divides up our training data set of 6 data points into mini-batches of 2 data points each\n",
    "mini_batches = [training_data[k:k+mini_batch_size] for k in xrange(0, n, mini_batch_size)]\n",
    "mini_batches\n",
    "\n",
    "# note that we can break up list comprehensions onto two lines to improve readability\n",
    "mini_batches = [training_data[k:k+mini_batch_size] \n",
    "                for k in xrange(0, n, mini_batch_size)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 complete\n",
      "Jake and Finn\n",
      "Finn and Jake\n",
      "Come on grab your 47 friends and 11.9557 princesses\n",
      "Come on grab your    47 friends and 11.9557 princesses\n",
      "Come on grab your 47.00 friends and 11.9557 princesses\n",
      "Come on grab your 47 friends and 11.96 princesses\n",
      "Come on grab your 47 friends and 11.96 princesses\n"
     ]
    }
   ],
   "source": [
    "# quick review of python print formatting\n",
    "# just printing a string and specifying the separator\n",
    "print \"Epoch {0} complete\".format(1)\n",
    "print '{0} and {1}'.format('Jake', 'Finn')\n",
    "print '{1} and {0}'.format('Jake', 'Finn')\n",
    "\n",
    "# play with numbers\n",
    "print \"Come on grab your {0} friends and {1} princesses\".format(47,11.9557) \n",
    "# print the first value (index of 0) to fill 5 spaces (d means integer)\n",
    "print \"Come on grab your {0:5d} friends and {1} princesses\".format(47,11.9557) \n",
    "\n",
    "# print the first value (index of 0) to fill 5 spaces, write it as a float, with precision of 2 decimal places\n",
    "print \"Come on grab your {0:5.2f} friends and {1} princesses\".format(47,11.9557) \n",
    "\n",
    "# write the sentence sensibly. These two statements are equivalent\n",
    "print \"Come on grab your {0:1d} friends and {1:4.2f} princesses\".format(47,11.9557)\n",
    "print \"Come on grab your {0} friends and {1:.2f} princesses\".format(47,11.9557)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Great! For each step in the training phase (each 'epoch'), you'll have noticed that we updated each mini_batch with the\n",
    "# update_mini_batch method. Now let's take a look at what this is \n",
    "# Intuitively, we want to gradually move the weights and biases towards the values that produce the correct output from the whole network\n",
    "# The rate at which we move towards this optimum depends on the learning rate, eta\n",
    "\n",
    "def update_mini_batch(mini_batch, eta):\n",
    "    # 'nabla' is the word for the upside down triangle symbol we've been using to denote gradient vectors\n",
    "    # np.zeros produces zeros in the same shape as the biases and weights\n",
    "    nabla_b = [np.zeros(b.shape) for b in biases]\n",
    "    nabla_w = [np.zeros(w.shape) for w in weights]\n",
    "    \n",
    "    for x, y in mini_batch:\n",
    "        \n",
    "        # we use backpropagation to get gradient vectors, treating as black box at the moment\n",
    "        delta_nabla_b, delta_nabla_w = backprop(x, y)\n",
    "        # we update the gradient vectors\n",
    "        nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "        nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "    \n",
    "    # now, we're updating the weights and bias vectors by the opposite of the gradients \n",
    "    # this updates the parameters to values that decrease the cost function\n",
    "    weights = [w-(eta/len(mini_batch))*nw \n",
    "                    for w, nw in zip(weights, nabla_w)]\n",
    "    biases = [b-(eta/len(mini_batch))*nb \n",
    "                    for b, nb in zip(biases, nabla_b)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 2)"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try out the zeros function\n",
    "np.zeros(5)\n",
    "np.zeros([2,5])\n",
    "# try out the shape function\n",
    "x = np.array([1,2,3])\n",
    "x.shape\n",
    "x = np.array([[1,2],[4,5]])\n",
    "x.shape\n",
    "x = np.array([[1,2,3],[4,5,6]])\n",
    "x\n",
    "x.shape\n",
    "# trying it out on our data set\n",
    "weights\n",
    "weights[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1\n",
      "[array([[ 0.00828323],\n",
      "       [ 0.00608295],\n",
      "       [ 0.00717254],\n",
      "       [ 0.00888058],\n",
      "       [ 0.0058908 ]]), array([[ 0.00812181]])]\n",
      "[array([[ 0.00140011,  0.00135424],\n",
      "       [ 0.00088149,  0.00652609],\n",
      "       [ 0.00344452,  0.00186034],\n",
      "       [ 0.00998321,  0.00495353],\n",
      "       [ 0.00735936,  0.0024171 ]]), array([[  6.66871643e-03,   3.91447667e-04,   3.51334990e-03,\n",
      "          4.17377002e-05,   2.14327866e-03]])]\n",
      "1 2\n",
      "[array([[ 0.00349216],\n",
      "       [ 0.00927524],\n",
      "       [ 0.00730807],\n",
      "       [ 0.00933622],\n",
      "       [ 0.00714726]]), array([[ 0.00473516]])]\n",
      "[array([[ 0.00474388,  0.00111355],\n",
      "       [ 0.00941205,  0.0032536 ],\n",
      "       [ 0.00799527,  0.00621469],\n",
      "       [ 0.00927401,  0.00823712],\n",
      "       [ 0.00846398,  0.00529215]]), array([[ 0.00758654,  0.0044922 ,  0.00248039,  0.00498939,  0.00518081]])]\n",
      "2 3\n",
      "[array([[ 0.00214758],\n",
      "       [ 0.00520195],\n",
      "       [ 0.00779013],\n",
      "       [ 0.0064365 ],\n",
      "       [ 0.0025817 ]]), array([[ 0.00923977]])]\n",
      "[array([[ 0.00615336,  0.00317071],\n",
      "       [ 0.00056597,  0.00662468],\n",
      "       [ 0.00221053,  0.0052209 ],\n",
      "       [ 0.00531456,  0.00423822],\n",
      "       [ 0.0011088 ,  0.00096889]]), array([[ 0.00555444,  0.0011454 ,  0.00247916,  0.00372718,  0.00696402]])]\n",
      "3 4\n",
      "[array([[ 0.00976801],\n",
      "       [ 0.00439796],\n",
      "       [ 0.00592323],\n",
      "       [ 0.00072451],\n",
      "       [ 0.00783616]]), array([[ 0.00763971]])]\n",
      "[array([[ 0.00711709,  0.00290593],\n",
      "       [ 0.00745245,  0.00295594],\n",
      "       [ 0.00725076,  0.00858117],\n",
      "       [ 0.00228333,  0.00504503],\n",
      "       [ 0.00823617,  0.00961312]]), array([[ 0.00170234,  0.00629318,  0.00764836,  0.00533411,  0.00899229]])]\n",
      "5 7\n",
      "[array([[ 0.00599801],\n",
      "       [ 0.00021562],\n",
      "       [ 0.00540819],\n",
      "       [ 0.00969784],\n",
      "       [ 0.00317303]]), array([[ 0.00379707]])]\n",
      "[array([[ 0.00797974,  0.00972379],\n",
      "       [ 0.00244453,  0.00032552],\n",
      "       [ 0.00772152,  0.00937647],\n",
      "       [ 0.00059169,  0.00139161],\n",
      "       [ 0.00158071,  0.00851809]]), array([[ 0.00960747,  0.00213532,  0.0004089 ,  0.00906627,  0.00609266]])]\n",
      "7 8\n",
      "[array([[ 0.00657211],\n",
      "       [ 0.00055301],\n",
      "       [ 0.00358671],\n",
      "       [ 0.00951541],\n",
      "       [ 0.00373216]]), array([[ 0.00578386]])]\n",
      "[array([[ 0.00440268,  0.00810837],\n",
      "       [ 0.00584461,  0.00545795],\n",
      "       [ 0.00199282,  0.00688202],\n",
      "       [ 0.00272442,  0.00667241],\n",
      "       [ 0.00611527,  0.00440858]]), array([[ 0.00531741,  0.00716707,  0.00065785,  0.00490681,  0.00888852]])]\n",
      "[array([[-0.4504397 ,  0.71953185],\n",
      "       [-0.91021052,  1.91802817],\n",
      "       [ 0.90842571,  0.19175359],\n",
      "       [-1.26648644,  0.58497478],\n",
      "       [-2.30254099,  1.53174999]]), array([[-1.61453843, -1.29761028, -0.25423925, -0.87761174,  0.11083983]])]\n",
      "[array([[ 0.26792374],\n",
      "       [ 1.06458723],\n",
      "       [-1.48769325],\n",
      "       [ 1.77043763],\n",
      "       [ 0.55292831]]), array([[ 0.97140333]])]\n"
     ]
    }
   ],
   "source": [
    "# Trying out the update mini batch function\n",
    "# the gradient vectors have the same shape as the bias and weights vectors\n",
    "# because every parameter (bias or weight) contributes to the cost function at some rate, i.e. has a dC/dp\n",
    "# I'll just set some parameters equal to arbitrary values so you can track how weights and biases are updated as \n",
    "# a function of the gradient vectors\n",
    "nabla_b = [np.zeros(b.shape) for b in biases]\n",
    "nabla_w = [np.zeros(w.shape) for w in weights]\n",
    "#print nabla_b\n",
    "\n",
    "eta = 0.05\n",
    "\n",
    "# next up, backprop returns a tuple that tells us how to update the weights and biases.\n",
    "# let's make up some values first\n",
    "for mini_batch in mini_batches:\n",
    "    \n",
    "    for x, y in mini_batch:\n",
    "        \n",
    "        print x,y\n",
    "\n",
    "        delta_nabla_b = [np.random.rand(x,y)*0.01 for x,y in (b.shape for b in biases)]\n",
    "        delta_nabla_w = [np.random.rand(x,y)*0.01 for x,y in (w.shape for w in weights)]\n",
    "        \n",
    "        # let's print out what the random delta nablas are\n",
    "        print delta_nabla_b\n",
    "        print delta_nabla_w\n",
    "\n",
    "        # we update the gradient vectors\n",
    "        nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "        nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "\n",
    "# updates the parameters \n",
    "weights = [w-(eta/len(mini_batch))*nw for w, nw in zip(weights, nabla_w)]\n",
    "biases = [b-(eta/len(mini_batch))*nb for b, nb in zip(biases, nabla_b)] \n",
    "\n",
    "print weights\n",
    "print biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's have a peak at backprop method, which will be covered a fair amount in Chapter 2 \n",
    "# this method returns (nabla_b, nabla_w) which is the gradient of the cost function\n",
    "# specifically, they are the partial derivatives of the cost function for each given parameter (each weight, each bias term)\n",
    "# we need to know these gradients for each parameter to figure out which direction to push them in\n",
    "def backprop(x, y):\n",
    "    \n",
    "    # we begin by knowing nothing about the partial derivatives - we initialise the gradient vectors (nablas) with zeros\n",
    "    nabla_b = [np.zeros(b.shape) for b in biases]\n",
    "    nabla_w = [np.zeros(w.shape) for w in weights]\n",
    "    \n",
    "    activation = x\n",
    "    activations = [x] # list to store all the activations, layer by layer\n",
    "    zs = [] # list to store all the z vectors, layer by layer\n",
    "    \n",
    "    # given some activation, we multiply by the weights and add the biases to get the activity at the next layer over\n",
    "    for b, w in zip(biases, weights):\n",
    "        z = np.dot(w, activation)+b\n",
    "        zs.append(z)\n",
    "        activation = sigmoid(z)\n",
    "        activations.append(activation)\n",
    "        \n",
    "        # we haven't covered this yet, but in back propagation the error is 'passed backwards' through the network\n",
    "        delta = cost_derivative(activations[-1], y) * \\\n",
    "            sigmoid_prime(zs[-1])\n",
    "        nabla_b[-1] = delta\n",
    "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "       \n",
    "        # l = 1 means the last layer of neurons, l = 2 is the second-last layer, and so on.\n",
    "        for l in xrange(2, self.num_layers):\n",
    "            z = zs[-l]\n",
    "            sp = sigmoid_prime(z)\n",
    "            delta = np.dot(weights[-l+1].transpose(), delta) * sp\n",
    "            nabla_b[-l] = delta\n",
    "            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
    "        return (nabla_b, nabla_w)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
